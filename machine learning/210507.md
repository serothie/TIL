# Natural Language Processing Ⅳ - document similarity

## Ⅰ. Introduction

> - 문서 유사도 측정
> - Bag of words, doc2vec
> - N-gram / RNN 기반 언어 모델

## Ⅱ. 문서 유사도 측정

문서는 문단, 문장, 형태소, 단어 등 다양한 요소와 이들의 상호 작용으로 구성되어 있다. 가장 기본 단위인 단어조차도 문서와 관련해서 다양한 정보를 포함한다.

이를테면 단어는 단순 형태소에서부터 문서의 키워드, 특정 정의를 담고 있는 개체명이거나 중의적 의미를 포함할 수도 있는 것이다. 단어의 상위 개념인 문장 또한 추가적인 정보를 제공한다. 문장에서 어떤 단어가 주어 또는 목적어이거나, 다른 문장과의 관계에 있는 등의 정보가 있을 것이다.

문서 내 문단, 문장 등과의 관계에서 가장 기본 단위인 단어를 활용하여 문서가 표현된다는 것에 기반하여, 문서 유사도를 측정하기 위해 단어 기준으로 생성한 문서 벡터 간의 코사인 유사도를 사용할 수 있을 것이다.

## Ⅲ. Bag of Words

### 1. 의의

문서를 이루는 단어들의 뭉치 속에서, 자주 발생하는 단어가 문서의 특징을 잘 나타낸다는 가정에 따라, 문서 내 `단어의 빈도수`를 기준으로 문서 벡터를 생성한다. 이 때 Bag of words 문서 벡터의 차원은 데이터 내 발생하는 `모든 단어의 개수`와 동일할 것이며 `합성어`의 경우에는 독립적인 단어 단위로 개별 처리한다.

### 2. Bag of N-grams

`N-gram`은 연속된 N개의 단어를 기준으로 텍스트 분석을 수행하는 것이다. 이에 따라 `Bag of N-grams`은 여러 N-gram들의 발생 빈도를 기준으로 문서 벡터를 표현한다.

### 3. TF-IDF

자주 발생하는 단어가 문서의 주요 내용 및 특징을 `항상` 효과적으로 표현하지는 않는다. 이를테면 문법적인 의미를 갖는 단어, 조사 등이 문서의 특징을 `효과적으로 표현`해주지는 않을 것이다.

따라서 TF-IDF(Term Frequency - Inverse Document Frequency)는 문서 내 `상대적으로 자주 발생하는 단어`가 더 중요하다는 점을 반영하여 문서의 특징과 유사도를 판단한다. 즉 TF-IDF 기반의 bag of words 문서 벡터는 단어의 상대적 중요성을 반영한다.

## Ⅳ. doc2vec

### 1. Bag of words 기반 문서 벡터의 장단점

벡터의 구성 요소가 `직관적`이라는 점은 `bag of words` 기반 문서 유사도 측정 기법의 가장 큰 장점이지만 텍스트 데이터의 양이 증가할수록, 문서 벡터의 차원이 증가하고, 대부분 단어의 빈도수가 0인 `희소(sparse) 벡터`가 생성됨에 따라 `메모리 제약에 있어 비효율성`이 발생한다는 단점이 있다.(`차원의 저주`)

### 2. doc2vec 의의

doc2vec은 문서 내 단어 간 문맥적 유사도를 기반으로 문서 벡터를 임베딩한다. 문서 내 단어의 임베딩 벡터를 학습하는 동시에 문서의 임베딩 또한 지속적으로 학습해 나간다. 이에 따라 유사한 문맥의 문서 임베딩 벡터는 인접한 공간에 위치하게 된다. doc2vec은 상대적으로 저차원의 공간에서 문서 벡터를 생성한다.

## Ⅴ. 언어 모델

### 1. 의의

언어 모델이란 주어진 문장이 텍스트 데이터에서 발생활 확률을 계산하는 모델이다. 이를 활용해서 자동 문장 생성이 가능하며 챗봇을 구현하는 핵심 요소 중 하나이기도 하다. 여기서 문장의 발생 확률은 단어가 발생할 조건부 확률의 곱으로 계산된다.

### 2. N-gram 기반 언어 모델

N-gram을 사용하여 단어의 조건부 확률을 근사한다. 이 때 각 N-gram 기반 조건부 확률은 데이터 내 `각 n-gram의 빈도수`로 계산한다. 문장 생성 시, 주어진 단어 기준 `최대 조건부 확률`의 단어를 다음 단어로 생성(추천)한다.

### 3. RNN 기반 언어 모델

```
출처: [엘리스 AI 트랙 1기](https://aitrack.elice.io/)
```
