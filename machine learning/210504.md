# Natural Language Processing Ⅰ - Introduction

## Ⅰ. Introduction

> - 자연어 처리, 텍스트 전처리
> - 단어 임베딩
> - word2vec, fastText

## Ⅱ. 자연어 처리

### 1. 의의

자연어 처리(Natural Language Processing, NLP)는 컴퓨터를 통해 인간의 언어를 분석 및 처리하는 인공지능의 한 분야이다. 문서 분류, 키워드 추출, 감정 분석 등에 이용되어 문서 요약, 기계 번역, 챗봇 등으로 활용 가능하다. 학습 가능한 데이터 양의 증가 및 연산 처리 속도의 발전으로 자연어 처리 또한 더욱 복잡한 머신러닝 알고리즘을 적용하여 발전할 수 있다.

## Ⅲ. 텍스트 전처리와 단어 임베딩

### 1. 의의 및 지프의 법칙

모델링을 위한 데이터 탐색 및 전처리 과정이다. 자연어 처리의 경우 데이터 탐색은 단어의 개수나 단어별 빈도수, 데이터 전처리의 경우 특수기호를 제거하거나 단어를 정규화하는 과정이 이에 해당한다.

공백을 기준으로 하는 가장 기본적인 `토큰화(Tokenization)`를 통해 주어진 텍스트를 각 단어 기준으로 분리하고, 소문자 처리 및 특수기호 제거를 통해 동일한 의미의 토큰은 동일한 형태를 갖도록 변환하는 등의 과정을 포함한다.

참고 : 대부분 단어 빈도수의 분포는 지프의 법칙을 따른다

### 2. 과정

#### (1). 특수 기호 제거

```python
import re

word = "123hello993 $!%eli$@ce^"
regex = re.compile('[^a-z A-Z]')

print(regex.sub('', word))
```

정규표현식 라이브러리를 활용한다.

#### (2). Stopword 제거

문법적인 기능을 지닌 단어 및 불필요하게 자주 발생하는 단어를 제거하는 과정이다.

```python
import nltk
from nltk.corpus import stopwords

sentence = ["the", "green", "egg", "and", "ham", "a", "an"]

stopwords = stopwords.words('english') # 리스트를 반환한다.
new_sentence = [word for word in sentence if word not in stopwords]

print(new_sentence)
# ["green", "egg", "ham"]

new_stopwords = ["none", "는", "가"] # 신규 stopword
stopwords = stopwords.words('English') # 리스트를 반환한다.

stopwords += new_stopwords
```

#### 3. Stemming

```python
import nltk
from nltk.stem import PorterStemmer

words = ["studies", "studied", "studying", "dogs", "dog"]
stemmer = PorterStemmer()

for word in words:
    print(stemmer.stem(word)) # studi, studi, studi, dog, dog
```

정규화 기법으로 자연어 분석 작업을 위해 만든 샘플 문서 집합인 `corpus(말뭉치)`의 단어수를 정규화하기(줄이기) 위해 어간을 추출하는 과정이다.

### 3. 단어 임베딩

컴퓨터 상에서는 텍스트를 포함한 모든 데이터를 0과 1로 처리한다. 따라서 자연어의 기본 단위인 단어를 `수치형 데이터`로 표현하기 위해 각 단어를 연속형 벡터로 표현하는 `단어 임베딩` 과정을 거친다.

비슷한 문맥에서 발생하는 단어는 유사한 의미를 지닌다는 점에서 유사한 단어의 임베딩 벡터는 벡터 공간에서 인접한 공간에 위치하게 되고 `임베딩 벡터 간 합과 차` 등 연산을 이용하여 단어의 의미적 특징을 분류하고 활용할 수 있다.

## Ⅳ. word2Vec & fastText

### 1. word2Vec

신경망을 통해 `단어 임베딩 벡터`를 학습하고 단어간 의미적 특징의 유사도 등을 분석할 수 있는 모델이다. 주어진 문맥에서 발생하는 단어를 예측하는 문제를 통해 임베딩 벡터를 학습하고 각 단어의 벡터는 해당 단어가 입력으로 주어졌을 때 계산되는 은닉층의 값을 사용한다.

```python
from gensim.models import Word2Vec

doc = [["서울에", "살고", "있는", "엘리스는", "강아지를", "좋아한다"]]

w2v_model = Word2Vec(min_count=1, window=2, vector_size=300)
w2v_model.build_vocab(doc)
w2v_model.train(doc, total_examples=w2v_model.corpus_count, epochs=20)

similar_word = w2v_model.wv.most_similar("엘리스는")

print(similar_word)
'''
[('있는', 0.05005083233118057), ('좋아한다', 0.03316839784383774),
('강아지를', 0.025744464248418808), ('서울에', 0.013042463921010494),
('살고', -0.0342760793864727)]
'''

score = w2v_model.wv.similarity("엘리스는", "좋아한다")

print(score)
# 0.03316839784383774
```

### 2. fastText

학습 데이터 내에 존재하지 않았던 데이터는 단어 벡터를 생성할 수 없는 문제점을 해소할 수 있다. 즉 어간 등을 중심으로 판단하여 학습 데이터에 존재하지 않았던 단어의 임베딩 벡터를 생성한다.

```python
from gensim.models import FastText

doc = [["서울에", "살고", "있는", "엘리스는", "강아지를", "좋아한다"]]

ft_model = FastText(min_count=1, window=2, vector_size=300)
ft_model.build_vocab(doc)
ft_model.train(doc, total_examples=ft_model.corpus_count, epochs=20)

similar_word = ft_model.wv.most_similar("엘리스는")
print(similar_word)
'''
[('좋아한다', 0.03110547922551632), ('살고', 0.015657681971788406),
('강아지를', -0.09297232329845428), ('서울에', -0.10255782306194305),
('있는', -0.10588616132736206)]
'''

new_vector = ft_model.wv["좋아한다고"]
print(new_vector)
# array([-5.8544584e-04, -1.5485507e-03, -1.3994898e-03, -9.1309723e-04, ...
```

```
출처: [엘리스 AI 트랙 1기](https://aitrack.elice.io/)
```
