# Deep Learning Introduction Ⅲ - Deep Learning Problems

## Ⅰ. Introduction

> - 모델 학습상 문제점
> - 학습 속도 문제와 최적화 알고리즘
> - 기울기 소실, 초기값 설정, 과적합

## Ⅱ. 딥러닝 모델의 문제점

### 1. 의의

실생활 데이터의 차원이 증가하고 구조가 복잡해지면서 다양한 기술적 문제가 발생한다. 딥러닝 모델 학습시 소요되는 시간이 함께 증가하는 `학습 속도 문제`, 더 깊고 넓은 신경망을 가진 모델이 학습하는 과정에서 출력값과 먼 데이터의 학습이 잘 되지 않는 `기울기 소실 문제`, 초기값 설정 방식에 따른 성능 차이가 나타나는 `초기값 설정 문제`, 학습 데이터에 모델이 과하게 최적화되어 새로운 테스트 데이터에 대한 모델 성능이 저하하는 `과적합 문제`들이 있다.

## Ⅲ. 학습 속도 문제

### 1. SGD : Stochastic Gradient Descent

GD(Gradient Descent) 는 시작 지점에서 기울기의 반대 방향으로 하강하면서 손실 함수(loss function)를 최소화하는 지점을 찾기 위한 가장 직관적인 방법이다. 이처럼 전체 데이터셋을 가지고 학습하게 되면 안정적이긴 하지만, 계산량과 학습 비용이 많아지게 된다.

이때 전체 데이터셋이 아닌, 무작위로 뽑은 데이터들에 대한 Gradient Descent를 진행하고, 이를 반복하며 정확도를 찾아 나가는 것을 SGD(Stochastic Gradient Descent)라고 한다.

전체 데이터(full-batch) 대신 일부 조그마한 데이터의 모음인 `mini-batch`에 대해서만 손실함수를 계산한다. 다소 부정확할 수 있지만 계산 속도가 훨씬 빠르기 때문에 같은 시간에 여러 스텝을 거쳐 학습할 수 있다.

```python
gd_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])

sgd_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])

word_num = 100
data_num = 25000

gd_model.compile(
    loss = 'binary_crossentropy',
    optimizer = 'sgd',
    metrics = ['accuracy', 'binary_crossentropy']
)
sgd_model.compile(
    loss = 'binary_crossentropy',
    optimizer = 'sgd',
    metrics = ['accuracy', 'binary_crossentropy']
)

gd_model.summary()
sgd_model.summary()

gd_history = gd_model.fit(
    train_data,
    train_labels,
    epochs = 20,
    batch_size = data_num,
    validation_data = (test_data, test_labels),
    verbose = 0
)

sgd_history = sgd_model.fit(
    train_data,
    train_labels,
    epochs = 20,
    batch_size = 500,
    validation_data = (test_data, test_labels),
    verbose = 0
)

scores_gd = gd_model.evaluate(test_data, test_labels)
scores_sgd = sgd_model.evaluate(test_data, test_labels)
```

### 2. Momentum

SGD는 계산 속도가 빠르지만 손실 함수(loss function)의 최솟값에 도달하는 동안 Gradient가 진동하여 최적값에 도달하기까지의 시간이 오래 걸리는 단점을 가지고 있다. 또한 SGD는 gradient 값 계산시 mini-batch의 데이터에 따라 gradient 방향이 문제가 될 수 있어 Learning Rate 설정이 어려워진다.

이에 따라 Momentum을 적용, 과거에 이동했던 방향을 기억하여 보정하는 방식을 적용할 수 있다.
모멘텀(momentum) 기법은 관성의 개념을 이용해 최적값에 좀 더 빠르게 도달할 수 있도록 돕는다.

```python
msgd_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])

msgd_opt = tf.keras.optimizers.SGD(lr = 0.01, momentum = 0.9)
msgd_model.compile(
    loss = 'binary_crossentropy',
    optimizer = msgd_opt,
    metrics = ['accuracy', 'binary_crossentropy']
)

msgd_model.summary()

msgd_history = msgd_model.fit(
    train_data,
    train_labels,
    epochs = 20,
    batch_size = 500,
    validation_data = (test_data, test_labels),
    verbose = 0
)

scores_msgd = msgd_model.evaluate(test_data, test_labels)
```

### 3. AdaGrad (Adaptive Gradient)

Adagrad(Adaptive Gradient) 최적화 알고리즘은 손실 함수(loss function)의 값을 최소로 만드는 최적의 가중치를 찾아내기 위해 learning rate를 조절해 하강하는 방법 중 하나이다.

변화가 작은 변수들의 Learning rate는 큰 값으로, 변화가 큰 변수들은 Learning rate를 작은 값으로 보정한다. 단 Gradient의 제곱값을 누적하는 것이기 때문에 학습이 진행될수록 값이 갱신되는 정도가 약해지는 단점이 있다.

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])

word_num = 100
data_num = 25000

adagrad_model = OPT_model(word_num)

adagrad_opt = tf.keras.optimizers.Adagrad(lr = 0.01, epsilon = 0.00001, decay = 0.4)
adagrad_model.compile(
    loss = 'binary_crossentropy',
    optimizer = adagrad_opt,
    metrics = ['accuracy', 'binary_crossentropy']
)

adagrad_model.summary()

adagrad_history = adagrad_model.fit(
    train_data,
    train_labels,
    epochs = 20,
    batch_size = 500,
    validation_data = (test_data, test_labels),
    verbose = 0
)

scores_adagrad = adagrad_model.evaluate(test_data, test_labels, verbose = 0)
```

### 4. RMSProp

RMSprop 최적화 알고리즘은 학습이 진행될수록 가중치 업데이트 강도가 약해지는 Adagrad의 단점을 보완하고자 제안된 방법이다.

RMSProp은 과거의 gradient 값은 잊고 새로운 gradient 값을 크게 반영해서 가중치를 업데이트한다.

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])

word_num = 100
data_num = 25000

rmsprop_model = OPT_model(word_num)

rmsprop_opt = tf.keras.optimizers.RMSprop(lr = 0.01)
rmsprop_model.compile(
    loss = 'binary_crossentropy',
    optimizer = rmsprop_opt,
    metrics = ['accuracy', 'binary_crossentropy']
)

rmsprop_model.summary()

rmsprop_history = rmsprop_model.fit(
    train_data,
    train_labels,
    epochs = 20,
    batch_size = 500,
    validation_data = (test_data, test_labels),
    verbose = 0
)

scores_rmsprop = rmsprop_model.evaluate(test_data, test_labels, verbose = 0)
```

### 5. Adam

Adam은 최적화 알고리즘 중 가장 발전된 기법입니다. RMSProp의 장점과 모멘텀(momentum)의 장점을 결합하고 함께 사용함으로써, 진행 방향과 learning rate 모두를 적절하게 유지하면서 학습할 수 있도록 고안되었다.

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, input_shape = (word_num,), activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])


word_num = 100
data_num = 25000

adam_model = OPT_model(word_num)

adam_opt = tf.keras.optimizers.Adam(lr = 0.01, beta_1 = 0.9, beta_2 = 0.999)
adam_model.compile(
    loss = 'binary_crossentropy',
    optimizer = adam_opt,
    metrics = ['accuracy', 'binary_crossentropy']
)

adam_model.summary()

adam_history = adam_model.fit(
    train_data,
    train_labels,
    epochs = 20,
    batch_size = 500,
    validation_data = (test_data, test_labels),
    verbose = 0
)

scores_adam = adam_model.evaluate(test_data, test_labels, verbose = 0)
```

## Ⅳ. 기울기 소실 문제

### 1. 의의

역전파(back propagation) 알고리즘은 우리의 목푯값과 실제 모델이 예측한 예측값이 얼마나 차이 나는지 구한 후, 오차값을 히든층의 뒤에서부터 역으로 전파해가며 가중치(weight)들을 업데이트하는 과정이다.

그러나 깊은 층의 모델에선 역전파 시에 전달되는 손실 함수(loss function)의 gradient 값에 활성화 함수인 sigmoid 함수의 0에 가까운 기울기 값이 계속해서 곱해지면서 결국 가중치 업데이트가 잘 안 되는 문제가 생기는데, 이것이 바로 기울기 소실 문제(Vanishing Gradient)이다.

### 2. 보완 방법

활성화 함수를 `sigmoid`로 설정하기 보다 `hidden layer`에서는 `relu`, `output layer`에서는 `tanh`로 설정하여 보완이 가능하다.

## Ⅴ. 초기값 설정 문제

### 1. 의의

초기값 설정 문제는 초기값에 따라 모델의 성능이 크게 좌우되는 문제점을 말한다. 가중치 초기화 문제는 활성화 함수의 입력값이 너무 커지거나 작아지지 않게 만들어주려는 것이 핵심이다.

초기화 설정 문제 해결을 위한 Naive한 방법으론 평균이 0, 표준 편차가 1인 표준 정규 분포를 이용해 초기화하는 방법과 평균이 0, 표준 편차가 0.01인 정규분포로 초기화하는 방법이 있다.

다만 나이브한 방식은 활성화 결과값들의 분포가 양단(0 또는 1) 또는 중앙(0.5)으로 치우처져 있어 학습이 안정적으로 이루어지지 않는다.

### 2. 보완 방법

#### (1). Xavier 초기화

가중치 초기화의 문제를 해결하기 위해 나온 방법의 하나인 Xavier 초기화 방법은 현재 일반적인 딥러닝 프레임워크들이 표준적으로 이용된다.

Xavier 초기화 방법은 앞 레이어의 노드가 n개일 때 표준 편차가 $$\frac{1}{\sqrt{n}}$$인 분포를 따르는 가중치 값들을 사용하는 것이다. 즉 표준 정규 분포를 입력 개수의 제곱근으로 나누어주면 된다.

따라서 Xavier 초기화 방법을 사용하면 앞 레이어의 노드가 많을수록 다음 레이어의 노드의 초깃값으로 사용하는 가중치가 좁게 퍼진다.

`sigmoid`, `tanh`와 같은 S자 함수에 적용하는 경우 출력 값들이 정규 분포 형태를 가지게 되어 안정적으로 학습이 가능하다.

#### (2). He 초기화

He 초기화 방법은 활성화 함수로 `ReLU`를 쓸 때 활성화 결괏값들이 한쪽으로 치우치는 문제를 해결하기 위해 나온 방법이다.

He 초기화 방법은 앞 레이어의 노드가 n개일 때 표준 편차가 $$\frac{\sqrt{2}}{\sqrt{n}}$$인 분포를 따르는 가중치 값들을 사용하는 것이다. 즉 표준 정규 분포를 입력 개수 절반의 제곱근으로 나누어준다.

`ReLU`는 음의 영역에 대한 함숫값이 0이라서 더 넓게 분포시키기 위해 $$\sqrt{2}$$배의 계수가 필요하다고 볼 수 있다.

## Ⅵ. 과적합

### 1. 의의

과적합(Overfitting)은 모델이 학습 데이터에만 너무 치중되어 학습 데이터에 대한 예측 성능은 좋으나 테스트 데이터에 대한 예측 성능이 떨어지는 경우이다.

모델이 과적합 되면 일반화되지 않은 모델이라고도 한다. 과적합이 발생하는 원인은 아래와 같다.

> - 데이터의 퍼진 정도, 즉 분산(variance)이 높은 경우
> - 너무 많이 학습 데이터를 학습시킨 경우 (epochs가 매우 큰 경우)
> - 학습에 사용된 파라미터가 너무 많은 경우
> - 데이터에 비해 모델이 너무 복잡한 경우
> - 데이터에 노이즈 & 이상치(outlier)가 너무 많은 경우

### 2. 보완 방법

#### (1). 정규화

#### (2). 드롭아웃

#### (3). 배치(Batch) 정규화
