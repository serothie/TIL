# Deep Learning Introduction Ⅱ - TensorFlow & Deep Learning

## Ⅰ. Introduction

> - 딥러닝 모델의 학습 방법
> - TensorFlow 기초 사용법과 모델 구현

## Ⅱ. 딥러닝 모델의 학습 방법

### 1. 의의

`Hidden Layer`가 3층 이상인 경우를 깊은 신경망이라는 의미에서 `Deep Learning`으로 구분한다. 모델을 구성하는 층인 `Layer`, 각 층을 구성하는 요소인 `Node/Unit`로 이루어져 있고 각 노드는 `가중치(Weight)`를 갖고 각기 다른 연결 강도로 이어져 있다.

딥러닝 모델은 `Loss function`을 최소화하는 방향으로 `Optimization` 알고리즘을 적용하여 학습한다. 손실 함수(Loss function)는 모델이 데이터를 학습하여 예측한 결과값과 실제값 간의 오차값, 최적화(Optimizatoin)는 오차값을 최소화하는 모델의 인자를 찾는 것이다.

즉 예측값과 실제값 간의 오차값을 최소화할 수 있는 모델의 인자를 찾는 알고리즘을 적용하여 모델을 학습시킨다.

### 2. 가장 기본적인 최적화 알고리즘 : Gradient Descent(GD)

#### (1). 의의

신경망들의 가중치에 따른 손실함수 `loss_function(W)`의 값을 최소화하기 위해 기울기를 활용하는 방법이다.

```
W(t+1) = W(t) - αΔloss_function(W)
α : learning rate
```

```python
import numpy as np

def linear_model(w0, w1, X):
    f_x = w0 + w1 * X
    return f_x

def mse_loss(f_x, y): # MSE
    ls = np.mean(np.square(y - f_x))

    return ls

def gradient_descent(w0, w1, X, y): # 편미분 적용
    gradient0 = 2 * np.mean((y - (w0 + w1 * X)) * (-1))
    gradient1 = 2 * np.mean((y - (w0 + w1 * X)) * (-1 * X))

    return np.array([gradient0, gradient1])

def GD_model():

    X = np.array([1,2,3,4]).reshape((-1,1))
    y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))

    # 파라미터 초기화
    w0 = 0
    w1 = 0

    # learning rate 설정
    lr = 0.001

    # 반복 횟수 1000으로 설정
    for i in range(1000):
        gd = gradient_descent(w0, w1, X, y)
        w0 = w0 - lr * gd[0]
        w1 = w1 - lr * gd[1]

        # 100회마다의 해당 loss와 w0, w1 출력
        if (i % 100 == 0):
            loss = mse_loss(linear_model(w0,w1,X),y)

            print(f"{i}번째 loss : {loss}")
            print(f"{i}번째 w0, w1 : {w0}, {w1}")

    return w0, w1
```

#### (2). 역전파

여러 개의 가중치들이 있을 때, 각 가중치의 기울기를 딥러닝에서는 `역전파(Backpropagation)`를 통해 구할 수 있다.

`역전파(Backpropagation)`는 실제값과 예측값 간의 차이를 구한 후, 그 오차값을 역방향으로 전파해가며 변수를 갱신하는 알고리즘이다.

```python
y = activation(w1*x1 + w2*x2 + B)
activation = lambda x: 1 if x >= 0 else 0
```

```python
import math

def sigmoid(x) :
    return 1 / (1 + math.exp(-x))

def getParameters(X, y) :
    f = len(X[0])
    w = [1] * f
    values = []

    while True :
        wPrime = [0] * f
        vv = [] # sigmoid를 통과한 r이 들어갈 빈 리스트

        for i in range(len(y)) :
            r = 0
            for j in range(f) :
                r = r + X[i][j] * w[j]

            v = sigmoid(r)
            vv.append(v)

            # w를 업데이트하기 위한 wPrime을 역전파를 이용해 구하는 식
            for j in range(f) :
                wPrime[j] += -((v - y[i]) * v * (1-v) * X[i][j])

        flag = False

        for i in range(f) :
            if abs(wPrime[i]) >= 0.001 :
                flag = True
                break

        if flag == False :
            break

        for j in range(f) :
            w[j] = w[j] + wPrime[j]

    return w
```

## Ⅲ. TensorFlow

### 1. 의의

딥러닝 모델의 학습과 추론을 위해 모델을 쉽게 구현하고 사용할 수 있는 라이브러리이다. 유연하고 효율적이며, 확장성있는 딥러닝 프레임어크로 대형 클러스터 컴퓨터부터 스마트폰까지 다양한 디바이스에서 동작시킬 수 있다.

Tensor란 딥러닝에서 다차원 배열 데이터를 의미한다. 플로우란 데이터의 흐름을 의미한다. TensorFlow의 계산은 데이터 플로우 그래프로 수행된다. 즉 그래프를 따라 데이터가 노드를 거쳐 흘러가며 계산된다.

### 2. Tensor 생성

#### (1). Constant Tensor

```python
import tensorflow as tf

def constant_tensors():
    t1 = tf.constant(5, shape = (1, 1), dtype = tf.int8)
    t2 = tf.zeros(shape = (3, 5), dtype = tf.int16)
    t3 = tf.ones(shape = (4, 3), dtype = tf.int8)

    return t1, t2, t3
```

#### (2). Sequence Tensor

```python
import tensorflow as tf

def sequence_tensors():
    seq_t1 = tf.range(1.5, 11, 4.5)
    seq_t2 = tf.range(2.5, 21, 4.5)

    return seq_t1, seq_t2
```

#### (3). Variable Tensor

```python
import tensorflow as tf

def variable_tensor():
    var_tensor = tf.Variable(initial_value = 100)
    W = tf.Variable(tf.ones(shape = (2, 2)), name = 'W')
    b = tf.Variable(tf.zeros(shape = (2,)), name='b')

    return var_tensor, W, b
```

### 3. Tensor 연산

```python
import tensorflow as tf

a = tf.constant(10, dtype = tf.int32)
b = tf.constant(3, dtype = tf.int32)

add = tf.add(a, b)
sub = tf.subtract(a, b)
mul = tf.multiply(a, b)
div = tf.truediv(a, b)
```

## Ⅳ. TensorFlow Deep Learning Model

### 1. Dateset : Epoch & Batch

`Epoch`는 전체 데이터 셋에 대해 한 번 학습을 완료한 상태를 의미하고 `Batch`는 나누어진 데이터셋의 크기이다. `iteration`는 `Epoch`를 나누어 실행하는 횟수를 의미한다. 총 데이터가 1,000개, `batch size`가 100 이라면 `1 iteration = 100개 데이터(batch)에 대한 학습` 이며 `1 epoch = 10 iteration`을 의미한다.

```python
import numpy as np
import tensorflow as tf

data = np.random.sample((100, 2))
labels = np.random.sample((100, 1))

# 데이터셋 생성
dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset = dataset.batch(32)
```

### 2. Model Construction : Keras

#### (1). 의의

TensorFlow의 패키지로 제공되는 고수준 API인 `Keras`를 통해 딥러닝 모델을 간단하고 빠르게 구현할 수 있다.

```python
import tensorflow as tf

# 모델 생성
model = tf.keras.model.Sequential()

# 모델의 각 레이어 구성
model.add(tf.keras.layers.Dense(units, activation))
# units: 레이어의 노드 수
# activation: 적용할 활성화 함수
```

#### (2). Input Layer

```python
# 입력의 형태와 차원을 지정하는 인자를 설정한다

model = tf.keras.model.Sequential([
    tf.keras.layers.Dense(10, input_dim=2, activation='sigmoid'),
    tf.keras.layers.Dense(10, activation='sigmoid'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

#### (3). Model Compile & Fit

```python
model.compile(
    loss = 'mean_squared_error',
    optimizer = 'SGD'
)

model.fit(
    dataset,    #x: data, y: labels
    epochs = 100
)
```

#### (4). Model Evaluate & Predict

```python
dataset_for_test = tf.data.Dataset.from_tensor_slices((data_for_test, labels_for_test))
dataset_for_test = dataset.batch(32)

model.evaluate(dataset_for_test)
predicted_labels_test = model.predict(dataset_for_test)
```

```
출처: [엘리스 AI 트랙 1기](https://aitrack.elice.io/)
```
