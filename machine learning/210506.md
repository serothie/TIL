# Natural Language Processing Ⅲ - Korean NLP

## Ⅰ. Introduction

> - 한국어 자연어 처리
> - KoNLPy & soynlp
> - 문장 유사도

## Ⅱ. 한국어 자연어 처리

### 1. 문제점

자연어 처리의 기본은 단어 추출에서 시작된다. 영어의 경우 문장의 의미, 구성 요소 및 특징을 파악할 수 있는 단어의 기본 단위가 명확하지만 한국어에서 단어의 기준은 명확하지 않다. 특히 교착어인 한국어에서 단어는 의미적 기능을 하는 부분과 문법적인 기능을 하는 부분의 다양한 조합으로 구성되어 있어 이를 구분하는 것이 중요하다.

## Ⅲ. KoNLPy

### 1. 형태소 분석

주어진 한국어 텍스트를 단어의 원형 형태로 분리해 주는 작업이다. KoNLPy는 Mecab, 한나눔, 꼬꼬마, Komoran, Open Korean Text 등 여러 한국어 형태소 사전을 기반으로 한국어 단어를 추출해 주는 파이썬 라이브러리이다.

### 2. 활용

```python
from konlpy.tag import Kkma

sent = "안녕 나는 엘리스야 반가워. 너의 이름은 뭐야?"
kkma = Kkma()

print(kkma.nouns(sent))
# ['안녕', '나', '엘리스', '너', '이름', '뭐']

print(kkma.pos(sent))
# [('안녕', 'NNG'), ('나', 'NP'), ('는', 'JX’),('엘리스', 'NNG'), ('야', 'JX’), ... ]

print(kkma.sentences(sent))
# ['안녕 나는 엘리스야 반가워. 너의 이름은 뭐야?']
```

```python
from konlpy.tag import Okt

sent = "안녕 나는 엘리스야 반가워. 너의 이름은 뭐야?"
okt = Okt()

print(okt.nouns(sent))
# ['안녕', '나', '엘리스', '너', '이름', '뭐']

print(okt.pos(sent))
# [('안녕', 'Noun'), ('나', 'Noun'), ('는', 'Josa’), ('엘리스', 'Noun’), ...

print(okt.pos(sent, stem = True))
# ... ('반갑다', 'Adjective’) ...
```

## Ⅳ. soynlp

### 1. 미등록 단어 문제

사전 기반의 단어 처리의 경우, 고유 명사, 신조어 등 미등록 단어 문제가 발생할 수 있다. soynlp에서 `단어는 연속으로 등자하는 글자의 조합`이며 글자 간 연관성이 높다는 가정 하에, 학습 데이터 내 `자주 발생하는 패턴을 기반으로 단어의 경계선을 구분`한다. 동시에 한국어의 어절은 `좌(의미적 기능) - 우(문법적 기능) 구조`로 2등분 할 수 있다ㄴ는 점을 활용해 미등록 단어를 추출한다.

### 2. 활용

```python
from soynlp.utils import DoublespaceLineCorpus
from soynlp.word import WordExtractor
from soynlp.noun import LRNounExtractor_v2

train_data = DoublespaceLineCorpus('./data_for_train.txt') # 데이터 기반 패턴 학습
noun_extractor = LRNounExtractor_v2()
nouns = noun_extractor.train_extract(train_data) # [할리우드, 모바일게임 ...
word_extractor = WordExtractor()
words = word_extractor.train_extract(train_data) # [클린턴, 트럼프, 프로그램
```

## Ⅴ. 문장 유사도

### 1. 의의

문장 사이의 유사한 정도는 `공통된 단어` 또는 `의미`를 기반으로 계산한다. 이중 `자카드(Jaccard)` 지수는 문장 간 공통된 단어의 비율로 문장 간 유사도를 정의한다. 한편 `코사인 유사도`는 문장 벡터 간의 각도를 기반으로 계산하는 방법이다. 이 때 벡터 사이의 각도는 벡터 간 내적을 사용하여 계산한다.

`유클리드 거리 계산`과 같은 다양한 거리 지표가 존재하지만, `코사인 유사도`는 코차원의 공간에서도 `벡터 간의 유사성`을 잘 보존하는 장점이 있다.

### 2. 자카드 지수

```python
def cal_jaccard_sim(sent1, sent2):
    # 각 문장을 토큰화 후 set 타입으로 변환
    words_sent1 = set(sent1.split())
    words_sent2 = set(sent2.split())

    # 공통된 단어의 개수를 intersection 변수에 저장
    intersection = words_sent1.intersection(words_sent2)

    # 두 문장 내 발생하는 모든 단어의 개수를 union 변수에 저장
    union = words_sent1.union(words_sent2)

    # intersection과 union을 사용하여 자카드 지수를 계산하고 float 타입으로 반환
    return float(len(intersection) / len(union))

# cal_jaccard_sim() 함수 실행 결과를 확인
print(cal_jaccard_sim(sent_1, sent_2))

# nltk의 jaccard_distance() 함수를 이용해 자카드 유사도 계산
sent1_set = set(sent_1.split())
sent2_set = set(sent_2.split())
nltk_jaccard_sim = 1 - nltk.jaccard_distance(sent1_set, sent2_set)

# 직접 정의한 함수와 결과 비교
print(nltk_jaccard_sim)
```

### 3. 코사인 거리

```python
from numpy import sqrt, dot
from scipy.spatial import distance
from sklearn.metrics import pairwise

sent_1 = [0.3, 0.2, 0.2133, 0.3891, 0.8852, 0.586, 1.244, 0.777, 0.882]
sent_2 = [0.03, 0.223, 0.1, 0.4, 2.931, 0.122, 0.5934, 0.8472, 0.54]
sent_3 = [0.13, 0.83, 0.827, 0.92, 0.1, 0.32, 0.28, 0.34, 0]

def cal_cosine_sim(v1, v2):
    # 벡터 v1, v2 간 코사인 유사도를 계산 후 반환
    top = dot(v1, v2)
    size1 = sqrt(dot(v1, v1))
    size2 = sqrt(dot(v2, v2))

    return top / (size1 * size2)

# 정의한 코사인 유도 계산 함수를 확인
print(cal_cosine_sim(sent_1, sent_2))

# scipy의 distance.cosine() 함수를 이용한 코사인 유사도를 계산
scipy_cosine_sim = 1 - distance.cosine(sent_1, sent_2)

# scipy를 이용해 계산한 코사인 유사도를 확인
print(scipy_cosine_sim)

# scikit-learn의 pairwise.cosine_similarity() 함수를 이용한 코사인 유사도를 계산
all_sent = [sent_1] + [sent_2] + [sent_3]
scikit_learn_cosine_sim  = pairwise.cosine_similarity(all_sent)

# scikit-learn을 이용해 계산한 코사인 유사도를 확인
print(scikit_learn_cosine_sim)
```

```
출처: [엘리스 AI 트랙 1기](https://aitrack.elice.io/)
```
