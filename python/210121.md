# Crawling Project : 코로나, 주가 데이터 크롤링

## Ⅰ. Introduction

> - 크롤링 기초를 적용한 코로나 데이터 및 네이버 주식 데이터 크롤링

## Ⅱ. 코로나 데이터 크롤링

### 1. Get Started

크롤링 함수(crawling)를 통해 해당 URL로부터 필요한 데이터를 전처리한다. 국가별 확진자, 사망자, 완치자의 데이터를 추려낸다. 해당 데이터는 딕셔너리 형태로 저장하되 결측값은 'N/A'로 지정하고 그 외 유효한 데이터는 정수형으로 출력한다.

### 2. 코드

```python
import requests
import re
from bs4 import BeautifulSoup


def crawling(soup):
    # 크롤링 함수
    result = list()
    table = soup.find('table', id='main_table_countries_today')
    tbody = table.find('tbody')
    trs = tbody.find_all('tr')

    for tr in trs:
        datas = tr.find_all('td', limit=7)
        result.append([])
        for data in datas:
            result[-1].append(data.get_text().replace(' ', '').replace('\n', '').replace('\t', '').replace(',', ''))
    result = result[7:]

    return result


def main():
    # soup 객체 생성
    html = requests.get("https://www.worldometers.info/coronavirus/")
    soup = BeautifulSoup(html.text, "html.parser")

    # 中 크롤링
    raw_data = crawling(soup)
    corona_related_data = dict()

    # 中 크롤링한 데이터를 딕셔너리로 저장
    for data in raw_data:
        corona_related_data[data[1]] = {'확진자': data[2], '사망자': data[4], '완치자': data[6]}

    # 데이터를 정수 자료형으로 표현 및 결측값은 문자열 'N/A'로 처리
    for country in corona_related_data:
        for value in corona_related_data[country]:
            if len(corona_related_data[country][value]) > 0 and corona_related_data[country][value] != 'N/A':
                corona_related_data[country][value] = int(corona_related_data[country][value])
            else:
                corona_related_data[country][value] = 'N/A'

    # 크롤링 결과 출력
    for key, value in corona_related_data.items():
        print(key, value)


if __name__ == "__main__":
    main()

```

## Ⅲ. 네이버 주식 데이터 크롤링

### 1. Get Started

크롤링 함수(crawling)를 통해 해당 URL로부터 필요한 데이터를 전처리한다. 종목별 종목명, 현재가, 등락률을 조회하고 상승한 종목의 종목명과 종목가를 출력한다.

### 2. 코드

```python
import requests
import re
from bs4 import BeautifulSoup

custome_header = {
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'}


# 기업에 대한 정보를 크롤링하는 함수
def crawling(soup):
    # 지시사항 1. 크롤링 데이터를 특수 문자 제거 후 리스트로 출력
    stock = list()
    tbody = soup.find('tbody')
    trs = tbody.find_all('tr')

    for tr in trs:
        datas = tr.find_all('td', limit=4)
        stock.append([])
        for data in datas:
            stock[-1].append(data.get_text().replace(' *', '').replace('\n', '').replace('\t', ''))
    stock = stock[:-2]

    print(stock)
    return stock


def main():
    # URL 크롤링
    url = "https://finance.naver.com/sise/sise_group_detail.nhn?type=upjong&no=235"
    req = requests.get(url, headers=custome_header)
    soup = BeautifulSoup(req.text, "html.parser")
    company_infos = crawling(soup)

    # 현재가가 오름차순이 되도록 data 딕셔너리를 출력
    # 등락률이 +인 종목을 찾아 딕셔너리 data에 저장, 현재가는 정수 자료형 처리
    data = dict()

    for company_info in company_infos:
        if re.findall('^\+', company_info[3]) != []:
            data[company_info[0]] = int(company_info[1].replace(',', ''))

    # 현재가가 오름차순이 되도록 data를 출력
    data_for_print = list()

    for key, value in data.items():
        data_for_print.append((key, value))

    data_for_print = sorted(data_for_print, key=lambda x: x[1])

    print(data_for_print)


if __name__ == "__main__":
    main()
```

## Ⅳ. 개선점

데어터의 결측값과 유효값을 분류하여 가공하는데 더 괜찮은 알고리즘이 없을지 고민해보고 이후 개선한다. 또는 데이터를 함수에서 저장하되 원하는 형태의 출력(리스트, 딕셔너리 등)을 더 세련되게 할 수 있는 방법으로 향후 수정한다.

```
출처: [엘리스 AI 트랙 1기](https://aitrack.elice.io/)
```
