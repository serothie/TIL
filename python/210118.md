# Python Crawling with requests/BeautifulSoup

## Ⅰ. Introduction

> - 크롤링 의의 및 관련 모듈 활용 방법

## Ⅱ. Get Started : Crawling & BeautifulSoup/requests

크롤링(crawling) 혹은 스크레이핑(scraping)은 웹 페이지를 그대로 가져와서 거기서 데이터를 추출해 내는 행위다. 크롤링하는 소프트웨어는 크롤러(crawler)라고 부른다.

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(open("index.html"), "html.parser")
print(soup)
```

BeautifulSoup 라이브러리는 HTML, XML, JSON등 파일의 구문을 분석하는 모듈이다. BeatifulSoup의 'html.parser' 옵션(디폴트)을 통해 HTML파일을 불러오고 BeautifulSoup 객체에게 HTML 파일을 분석하고 명시하게 할 수 있다.

```python
import requests
from bs4 import BeautifulSoup

url = "https://www.naver.com"

req = requests.get(url)
soup = BeautifulSoup(req.text, 'html.parser')
```

requests 라이브러리는 Python에서 HTTP 요청을 보낼 수 있는 모듈이다. 정보를 조회(웹페이지 접속, 검색)하기 위한 GET 요청과 정보를 생성, 변경(로그인, 메일 삭제 등)하기 위한 POST 요청을 의미한다.

## Ⅲ. BeautifulSoup / requests 라이브러리

### 1. BeutifulSoup

```python
from bs4 impoert BeautifulSoup

soup = BeautifulSoup(open('index.html'), 'html.parser')

p = soup.find('p')           # 처음 등장하는 p 태그를 찾아 반환
a_list = soup.find_all('a')  # 모튼 a 태그를 찾아 리스트로 반환

# 특정 클래스, 아이디를 가진 태그 추출
div = soup.find('div', class_ = 'main_article')
span = soup.find('span', id = 'bgimage')

# 태그가 가지고 있는 텍스트를 추출
result = soup.find('div', class_ = 'main_article').find('p').get_text()
```

### 2. requests

```python
import requests

# 지정한 URL에 GET 요청을 보내고 서버에서는 요청을 받아 처리 후 result 변수에 응답을 보낸다.
url = "http://www.google.com"
req = requests.get(url)

print(req.status_code)  # 요청 성공시 200 출력, 실패시 404
print(req.text)         # 해당 웹 사이트의 HTML 추출

# requests와 BeautifulSoup을 조합하여 웹 페이지의 HTML 분석 가능
soup = BeautifulSoup(req.text, 'html.parser')
```

## Ⅳ. Query를 이용한 다중 페이지 Crawling

### 1.requests params

```python
import requests

url = 'http://www.google.com/search'
req = requests.get(url, params = {'q', '검색어'})
```

requests의 get 메소드로 요청을 보낼 때 params 매개변수에 딕셔너리를 전달하여 쿼리를 지정할 수 있다. 반복문이나 문자열 조작을 이용해 requests를 이용한 정보 요청을 하고 이를 가공하여 원하는 정보들을 모을 수 있다.

### 2. Tag Attribute

```python
from bs4 import BeautifulSoup

div = soup.find('div')
print(div.attrs)      # 어떤 태그의 속성(id, class, href 등)을 딕셔너리로 확인
print(div['class'])   # attrs 딕셔너리의 키로 인덱싱하여 태그의 속성에 접근 가능하다.

a = soup.find('a')
href_url = a['href']  # 링크에 접근하는 경우

# 해당 태그가 포함하고 있는 태그들을 리스트로 조회
div_children = soup.find('div').childeren

# 각 태그의 이름을 출력, 태그가 존재하지 않는 경우 None
for div_child in div_children:
    print(child.name)
```

## Ⅴ. 결론

```

```

```python
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.368'}
url = "http://openapi.seoul.go.kr:8088/4349786750726d6139356b486b7272/json/Corona19Status/1/1000/"
response = requests.get(url, headers=headers)
data = response.json()

def parse_result(**kwargs):
    date_dict = {}
    rows = data['Corona19Status']['row']

    for row in rows:
        date = row[kwargs['target']]
        if date in date_dict:
            date_dict[date] += 1
        else :
            date_dict[date] = 1

    for k, v in date_dict.items():
        print(k, ':', v)

target_list = ['CORONA19_DATE', "CORONA19_AREA"]

for target in target_list:
    parse_result(target=target)
```
